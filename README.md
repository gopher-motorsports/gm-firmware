# Firmware Repo Structure Proposal
## Motivation and Requirements
Historically, the electrical subteam's use of version control has been limited in scope to the module level. This worked fine in previous electrical systems which consisted mainly of unrelated modules that would send a very small number of CAN signals to one another. In the system's current architecture, this is no longer the case.  
The proposed firmware version control structure is based on a different approach: instead of the focus being on individual modules, the entire car's worth of module's will be stored, built, and tested together in an effort to mitigate integration issues. The primary goals of this system are as follows:
1. Minimal/no impact to current developer workflows
    - System must integrate with existing repo structures
    - System must integrate with STM32CUBEIDE build process file structure (most devs use this for development, cannot break this use case)
    - Exceptions to this would include modifications that will be required no matter what for integration of GopherCAN
    - A result of this is a continued reliance on autogenerated `makefiles`. GNU make isn't the greatest build system in the world, but it will work fine for our current needs
2. Must provide common build/test environment locally (cleanroom)
    - Need to ensure no possibility of "well it builds on MY machine!" issues
3. Must integrate with CI runners
4. Must automate as much as possible to make the cost of maintaining such a system low  
## High-Level Overview
The proposed version control system consists of a single repo for all modules, as well as GopherCAN, c libraries, and validation infrastructure. It will be organized as follows:
- gm-firmware
    - components
        - acm
        - dam
        - dlm
        - pdm
        - tcm
    - gophercan
    - libraries
    - validation

This simple directory structure allows for clean integration of various build and automation tools, as well as providing a consistent way to provide paths to various directories in build scripts.
## Build System
The build for each module will be orchestrated by CubeIDE-generated `makefiles`. This is a little clunky, but ensures compatibility with the CubeIDE build process. Building will normally take place inside of a Docker container, which will be the only dependency users will need to setup before working on the repo. Builds for each module (in this example, the ACM) will be triggered with a command format like `docker-compose run acm`. This will spinup a prebuilt "build" container with a locked-down toolchain version that will be consistent across all builds team-wide. This will eliminate the possibility of developers running into issues because they are using different versions of a toolchain/dependencies.
## CI Integration
Because Docker will be used as the primary build method, CI integration will be relatively straightforward. The use of a common container also ensures that the CI builds will be identical to the local ones. An overall goal will be that anything we do on CI can also be performed locally, so devs will be able to replicate CI failures locally without any trouble.
## Branch Strategy
One of the impacts of using a monolithic repo structure is the potentially cascading effect that failures in one module may have on other modules. Consider a simple case where someone is making a modification to a few GopherCAN signals to add some PDM functionality. The developer ensures that the PDM build passes, and merges their changes into the mainline. Unknown to them, this change breaks the build on the TCM which was relying on the previous format of the signals. Now the TCM team must scramble to understand and fix this bug due to the fact that now the mainline build of TCM breaks, stalling other progress.  
The proposed solution to this is a three-tier branch strategy:
- In the highest tier is the `main` branch. This is the least-frequently updated branch but guarantees that all modules, libraries, and GopherCAN networks build (and potentially test) successfully. It can be though of as the most "protected" branch.
- In the next tier are the module-specific branches. These would consist of `ACM`, `DAM`, `DLM`, `PDM`, `TCM`, `gophercan`, and `library` branches. These branches have less strict requirements, only requiring that the referenced module/library builds successfully.
- In the final tier are the user branches. These branches have no requirements and are allowed to have build breakages.  

To understand how this branching strategy works in practice, consider the initial example played out on a repo with this structure. Instead of merging straight to `main`, the developer would instead merge to `PDM` after ensuring the PDM build passed. Once there, to propagate the changes to `main`, the developer would have to prove that all other modules/libraries build successfully. In this stage, the aforementioned bug would be caught before causing stoppages on the TCM team. The bug would be fixed, all of the module builds would be confirmed to pass, and the `PDM` branch would merge into `main`
## Automerging
An astute reader will have realized there is one major flaw in this system: how will changes on one module branch propagate into the other module branches? Ah dear reader, that is where the automation begins do do the heavy lifting. Aside from passing all module/library builds, to merge into `main` a module branch must also pass so-called automerger "dry runs". These will consist of a very simple job that will attempt to perform a merge of the source module branch into all of the other module branches. In the example, this would mean a test merge of `PDM` into `ACM`, `DAM`, `gophercan`, and so forth. In the event that this "dry run" merge succeeds, all is well. If there are merge conflicts, the job will fail and prompt the user to run something like
```
git checkout <source branch>
git pull origin <failing branch>
git merge <failing branch>
```
to fix the merge conflicts. Once all dry runs pass and the developer is able to merge into `main`, a job will kick off when the `main` merge is successful which will automatically merge the source module branch into all the others without any manual intervention. This merge is not guaranteed to be "safe" however, and changes in the source may cause breakages in the destination due to the fact that the destination may be different from `main`, but this has the effect of uncovering those issues as quickly as possible. This also allows for timely propagation of new features between modules without any manual merging or rebasing required.